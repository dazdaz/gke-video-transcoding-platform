# metric-server-components.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-metrics-exporter
  namespace: default
data:
  metrics_exporter.py: |
    #!/usr/bin/env python3
    import time
    import subprocess
    import psutil
    from prometheus_client import start_http_server, Gauge, Histogram
    import logging
    import os

    logging.basicConfig(level=logging.INFO)

    # Define metrics
    gpu_util = Gauge('custom_gpu_utilization', 'GPU utilization percentage', ['gpu_id'])
    nvenc_sessions = Gauge('custom_nvenc_sessions', 'Number of NVENC sessions', ['gpu_id'])
    disk_read_rate = Gauge('custom_disk_read_kb_per_sec', 'Disk read rate in KB/s', ['device'])
    disk_write_rate = Gauge('custom_disk_write_kb_per_sec', 'Disk write rate in KB/s', ['device'])
    video_queue_depth = Gauge('video_queue_depth', 'Number of videos in processing queue')
    processing_latency = Histogram('video_processing_duration_seconds', 'Video processing duration')

    def find_nvidia_smi():
        """Find nvidia-smi in common locations"""
        possible_paths = [
            '/usr/bin/nvidia-smi',
            '/usr/local/nvidia/bin/nvidia-smi',
            '/home/kubernetes/bin/nvidia/bin/nvidia-smi',
            '/usr/local/bin/nvidia-smi'
        ]

        for path in possible_paths:
            if os.path.exists(path):
                logging.info(f"Found nvidia-smi at: {path}")
                return path

        # Try to find it using which command
        try:
            result = subprocess.run(['which', 'nvidia-smi'], capture_output=True, text=True)
            if result.returncode == 0:
                path = result.stdout.strip()
                logging.info(f"Found nvidia-smi via which: {path}")
                return path
        except:
            pass

        logging.warning("nvidia-smi not found in standard locations")
        return None

    nvidia_smi_path = None

    def collect_gpu_metrics():
        global nvidia_smi_path

        if nvidia_smi_path is None:
            nvidia_smi_path = find_nvidia_smi()
            if nvidia_smi_path is None:
                logging.error("nvidia-smi not available, skipping GPU metrics")
                return

        try:
            result = subprocess.run(
                [nvidia_smi_path, '--query-gpu=index,utilization.gpu,encoder.stats.sessionCount',
                 '--format=csv,noheader,nounits'],
                capture_output=True, text=True
            )

            if result.returncode != 0:
                logging.error(f"nvidia-smi failed: {result.stderr}")
                return

            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                parts = line.split(', ')
                if len(parts) >= 3:
                    gpu_id = parts[0]
                    util = float(parts[1]) if parts[1] and parts[1] != '[N/A]' else 0
                    sessions = int(parts[2]) if parts[2] and parts[2] != '[N/A]' else 0
                    gpu_util.labels(gpu_id=gpu_id).set(util)
                    nvenc_sessions.labels(gpu_id=gpu_id).set(sessions)
        except Exception as e:
            logging.error(f"Error collecting GPU metrics: {e}")

    def collect_disk_metrics():
        try:
            disk_io = psutil.disk_io_counters(perdisk=True)
            for disk, counters in disk_io.items():
                disk_read_rate.labels(device=disk).set(counters.read_bytes / 1024)
                disk_write_rate.labels(device=disk).set(counters.write_bytes / 1024)
        except Exception as e:
            logging.error(f"Error collecting disk metrics: {e}")

    if __name__ == '__main__':
        # Start Prometheus metrics server
        start_http_server(8000)
        logging.info("Metrics server started on port 8000")

        while True:
            collect_gpu_metrics()
            collect_disk_metrics()
            time.sleep(10)

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: custom-metrics-exporter
  namespace: default
spec:
  selector:
    matchLabels:
      app: custom-metrics-exporter
  template:
    metadata:
      labels:
        app: custom-metrics-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
    spec:
      hostNetwork: true  # Use host network to access GPU
      hostPID: true      # Access host processes
      containers:
      - name: metrics-exporter
        image: python:3.9-slim
        command:
        - /bin/sh
        - -c
        - |
          # Install dependencies first
          pip install prometheus-client psutil
          # Then run the metrics exporter
          python3 /app/metrics_exporter.py
        ports:
        - containerPort: 8000
          name: metrics
          hostPort: 8000
        volumeMounts:
        - name: metrics-script
          mountPath: /app
        # Mount only the nvidia bin directory
        - name: nvidia-install-dir
          mountPath: /home/kubernetes/bin/nvidia
          readOnly: true
        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "utility"
        - name: PATH
          value: "/home/kubernetes/bin/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
        - name: LD_LIBRARY_PATH
          value: "/home/kubernetes/bin/nvidia/lib64"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        securityContext:
          privileged: true
          capabilities:
            add:
            - SYS_ADMIN
      volumes:
      - name: metrics-script
        configMap:
          name: custom-metrics-exporter
      # Mount only the nvidia directory that exists
      - name: nvidia-install-dir
        hostPath:
          path: /home/kubernetes/bin/nvidia
          type: Directory  # Changed from DirectoryOrCreate to Directory
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-tesla-t4
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

---
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: custom-metrics-monitoring
  namespace: default
spec:
  selector:
    matchLabels:
      app: custom-metrics-exporter
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
